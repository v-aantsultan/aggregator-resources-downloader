// TODO: Name for logging
app.name = aggr-ana-gv-salesb2c-idr

// In ISO8601 Format
// e.g. 2019-01-01T00:00:00+08:00
start-date: ${start.date}
end-date: ${end.date}

// Flattener bucket
flattener-src: ${flattener.src}

// The bucket name of aggregator output (CSV)
aggregator-dest: ${aggregator.dest}

// The schema name in data warehouse
schema-name: ${schema.name}

// The table name in data warehouse
table-name: ${table.name}

// The partition key to split the reports into many deltas
// So the key is `booking_issue_date`, which is an indexed column in the warehouse table
partition-key: ${partition.key}

statusmanager {
  url = ${statusmanager.url}
  username = ${statusmanager.username}
  password = ${statusmanager.password}
  timeout = 10s
}

// Only needed in Local mode for spark to write to s3
// Set via environment variables
aws-access-key-id: ${?AWS_ACCESS_KEY_ID}
aws-secret-access-key: ${?AWS_SECRET_ACCESS_KEY}
aws-session-token: ${?AWS_SESSION_TOKEN}
