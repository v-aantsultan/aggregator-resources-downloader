app.name = aggr-ana-ic-paylater-waterfall

// In ISO8601 Format
// e.g. 2019-01-01T00:00:00Z
start-date: "2022-06-27T00:00:00Z"
end-date: "2022-11-07T00:00:00Z"

// Download datalake locally from https://github.com/traveloka/tvlk-eci-scripts/tree/master/datalake_sync
// Flattener bucket, consider using local path once datalake is downloaded locally
flattener-src: "s3a://ecidtpl-data-warehouse-674080593013-d94c46ee172a319c/data_warehouse"
flattener-srcdtl: "s3a://ecidtpl-datalake-674080593013-6f9e42b6241969df"

// The bucket name of aggregator output (CSV)
aggregator-dest: ${HOME}"/ecidtpl-aggregrator-output"

// The schema name in data warehouse (for destination)
schema-name: "anaplan"

// The table name in data warehouse (for destination)
table-name: "ic_paylater_r001"

// The partition key to split the reports into many deltas
// So the key is `booking_issue_date`, which is an indexed column in the warehouse table
partition-key: "report_date"

statusmanager {
  url = "localhost"
  username = "application"
  password = "application"
  timeout = 10s
}

// Only needed in Local mode for spark to write to s3
// Set via environment variables
aws-access-key-id: ""
aws-secret-access-key: ""
aws-session-token: ""

slack {
  channel: ""
  token: ""
}