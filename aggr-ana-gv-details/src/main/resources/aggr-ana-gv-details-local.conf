// TODO: Fill up local config file
app.name = aggr-ana-gv-details

// In ISO8601 Format
// e.g. 2019-01-01T00:00:00+08:00
start-date: "2021-09-30T00:00:00Z"
end-date: "2021-10-31T00:00:00Z"

// Download datalake locally from https://github.com/traveloka/tvlk-eci-scripts/tree/master/datalake_sync
// Flattener bucket, consider using local path once datalake is downloaded locally
flattener-src: "s3a://ecidtpl-data-warehouse-307648842078-c5fc76de0f09ad0d/data_warehouse/"
flattener-src-dtl: "s3a://ecidtpl-datalake-307648842078-78efbb49fefabe80/"
flattener-src-local: ${HOME}"/DWH/"

// The bucket name of aggregator output (CSV)
aggregator-dest: ${HOME}"/output"

// The schema name in data warehouse
schema-name: "anaplan"

// The table name in data warehouse
table-name: "gv_details"

// The partition key to split the reports into many deltas
// So the key is `booking_issue_date`, which is an indexed column in the warehouse table
partition-key: "date"

statusmanager {
  url = "localhost:5432"
  username = "application"
  password = "application"
  timeout = 10s
}

// Only needed in Local mode for spark to write to s3
// Set via environment variables
aws-access-key-id: "ASIAUPIKMZFPAQDH6D7X"
aws-secret-access-key: "YGUJflk7Ovj206AXeb1SQL3Gc8Kx8KbIdgJBnVYP"
aws-session-token: "FwoGZXIvYXdzELn//////////wEaDApo51ZHeAQFZWsQ7iKsAWxBgzA+7/I6qyfA74hX6xsgo2jAGNJ6xxnPnRsf9rn+0uEPz9PSn7jkJUwdeYxF4kwFtIGoet2rmwtPD2Omd56QNh1QyHbodX3BXRySg8FVsrxWGIwW0DbAGQwq6mbQgDxhELRjJoGNidnQ1iNW1pUX6f12Lan5H8Y3uD+AF6myPcX3ZbjkqxR/OHw4DSqeC78rtgjB6HEACAA+KrIPBVwVAfMQdjHxUMkZQCAo//D8jAYyLeeO2yk9IvPc1E5Ay6nILIUlwU01cKiWR/ZmDZiPWAZY5yg3ennuHmyPAXlwbg=="
